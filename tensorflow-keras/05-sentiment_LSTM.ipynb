{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7dd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load IMDB dataset\n",
    "# Keras provides IMDB dataset with reviews already tokenized into integers.\n",
    "# num_words=10000 → keep only the 10,000 most frequent words in the dataset.\n",
    "num_words = 10000\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)\n",
    "\n",
    "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape)\n",
    "print(\"Example review (first 20 tokens):\", x_train[0][:20])\n",
    "print(\"Label (1=positive, 0=negative):\", y_train[0])\n",
    "\n",
    "# Step 3: Pad sequences\n",
    "# Reviews have different lengths → pad/truncate to fixed size (200 words).\n",
    "# pad_sequences ensures all sequences are the same length → needed for LSTMs.\n",
    "maxlen = 200\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(\"Padded review shape:\", x_train.shape)\n",
    "\n",
    "# Step 4: Build the LSTM model\n",
    "# - Embedding: maps word IDs → dense vectors (128 dimensions)\n",
    "# - LSTM: recurrent network with 64 memory units\n",
    "# - Dense: final classification layer with sigmoid → probability between 0 and 1\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=num_words, output_dim=128, input_length=maxlen),\n",
    "    layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "# - Loss: binary_crossentropy (because output is 0/1)\n",
    "# - Optimizer: Adam (common default optimizer)\n",
    "# - Metrics: track accuracy\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Step 5: Train model\n",
    "# validation_split=0.2 → hold out 20% of training data for validation\n",
    "# batch_size=64 → train in mini-batches of 64 samples\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 6: Evaluate on test data\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {acc:.4f}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Step 7: Plot training vs validation accuracy\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Example prediction\n",
    "# Pick one test review, check model prediction\n",
    "sample_idx = 0\n",
    "sample_review = x_test[sample_idx].reshape(1, -1)\n",
    "prediction = model.predict(sample_review)[0][0]\n",
    "\n",
    "print(\"True label:\", y_test[sample_idx])\n",
    "print(\"Predicted probability (positive):\", prediction)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
