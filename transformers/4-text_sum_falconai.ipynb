{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e143c339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\enisw\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09dfffd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA version used to compile PyTorch: 12.8\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version used to compile PyTorch: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87050919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Falconsai/text_summarization on cuda\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Falconsai/text_summarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Explicitly move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using model {model_name} on {device}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acaaa137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['article', 'highlights', 'id'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['article', 'highlights', 'id'],\n",
       "     num_rows: 200\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['article', 'highlights', 'id'],\n",
       "     num_rows: 11490\n",
       " }))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use only a small subset for faster experimentation\n",
    "raw_datasets = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "# Take only first 1000 examples for train and 200 for validation\n",
    "small_train = raw_datasets[\"train\"].select(range(1000))\n",
    "small_val = raw_datasets[\"validation\"].select(range(200))\n",
    "raw_datasets[\"train\"] = small_train\n",
    "raw_datasets[\"validation\"] = small_val\n",
    "raw_datasets[\"train\"], raw_datasets[\"validation\"], raw_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70230d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08180721149d4833a2bb951cabce9e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\enisw\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59a0663941d40ef899284be2106c975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ff786853004359a96da10204f2f897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 256\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"article\"]\n",
    "    # you can prefix with a task indicator (optional for some models)\n",
    "    inputs = [\"summarize: \" + doc for doc in inputs]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # tokenize summaries (targets)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"highlights\"],\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7458f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # decode, skip special tokens\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Sometimes one summary is empty string after decoding â†’ fix that\n",
    "    decoded_preds = [pred if pred.strip() != \"\" else \" \" for pred in decoded_preds]\n",
    "    decoded_labels = [label if label.strip() != \"\" else \" \" for label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # Fix: Handle different ROUGE metric formats\n",
    "    if isinstance(result[\"rouge1\"], dict) and \"mid\" in result[\"rouge1\"]:\n",
    "        # Old format with .mid attribute\n",
    "        result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    else:\n",
    "        # New format - direct float values\n",
    "        result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # optional: add generation length\n",
    "    prediction_lens = [len(tokenizer(p).input_ids) for p in decoded_preds]\n",
    "    result[\"gen_len\"] = sum(prediction_lens) / len(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68517742",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./summarization_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,  # Start smaller if GPU memory is limited\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    dataloader_num_workers=0,  # Set to 0 on Windows to avoid multiprocessing issues\n",
    "    remove_unused_columns=False,\n",
    "    no_cuda=False,  # Explicitly enable CUDA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb18447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enisw\\AppData\\Local\\Temp\\ipykernel_6720\\1485828738.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb62236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.445300</td>\n",
       "      <td>1.803423</td>\n",
       "      <td>10.361136</td>\n",
       "      <td>3.670638</td>\n",
       "      <td>8.288115</td>\n",
       "      <td>8.226439</td>\n",
       "      <td>8.790000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=2.4535074462890627, metrics={'train_runtime': 9.5529, 'train_samples_per_second': 104.68, 'train_steps_per_second': 13.085, 'total_flos': 67670900736000.0, 'train_loss': 2.4535074462890627, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a6569fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1437' max='1437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1437/1437 03:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0443038940429688, 'eval_rouge1': 23.065755941942268, 'eval_rouge2': 11.271395965585073, 'eval_rougeL': 19.114765598704807, 'eval_rougeLsum': 19.126732835249, 'eval_gen_len': 16.917493472584855, 'eval_runtime': 208.9027, 'eval_samples_per_second': 55.002, 'eval_steps_per_second': 6.879, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "745cde70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Recent advances in AI have led to huge leaps in productivity. The latest advances in AI have led to huge leaps in productivity. The latest advances in AI have led to huge leaps in productivity.', 'a new kind of exoplanet orbiting a distant star. Scientists discovered a new kind of exoplanet orbiting a distant star.']\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"The recent advances in AI have led to huge leaps in productivity...\",\n",
    "    \"Scientists discovered a new kind of exoplanet orbiting a distant star...\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer([\"summarize: \" + t for t in texts], return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_length=150,\n",
    "    min_length=40,\n",
    "    num_beams=4,\n",
    "    length_penalty=2.0,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print([tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "032b58c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam summary:\n",
      "Recent advances in AI have led to huge leaps in productivity. The latest advances in AI have led to huge leaps in productivity. The latest advances in AI have led to huge leaps in productivity.\n",
      "\n",
      "Sampling summary:\n",
      "Recent advances in AI have led to huge leaps in productivity.... The recent advances in AI have led to huge leaps in productivity..........\n"
     ]
    }
   ],
   "source": [
    "# Same input, but sampling\n",
    "generated_sampling = model.generate(\n",
    "    **inputs,\n",
    "    max_length=150,\n",
    "    min_length=40,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Beam summary:\")\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "print(\"\\nSampling summary:\")\n",
    "print(tokenizer.decode(generated_sampling[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
