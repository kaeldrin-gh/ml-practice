{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c468a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57040b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807bd124df1d42109a44469279b632b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\enisw\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\enisw\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e4c6193ae14771aa34bfa441094e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5fe576a936419bbae77e0e645c2e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bc14372d464e10b7b403b6e63da092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac8a3c54df34201a6d63ba14d578769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48472483afd1424da5c4a87eea888fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e418f43f1a0449baff8ac2e583501e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8460587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  818,   262,  2003,    11, 11666,  4430,   481]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Input prompt\n",
    "prompt = \"In the future, artificial intelligence will\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"Input IDs: {inputs['input_ids']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6be59842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy output:\n",
      "In the future, artificial intelligence will be able to do things like search for information about people, and to do things like search for information about people.\n",
      "\n",
      "\"We're going to see a lot of things that are going to be very interesting in\n"
     ]
    }
   ],
   "source": [
    "# Greedy decoding: pick highest probability token each step\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,   # total length including prompt\n",
    "    do_sample=False  # greedy decoding\n",
    ")\n",
    "\n",
    "print(\"Greedy output:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e50b3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "In the future, artificial intelligence will also be used to develop a better way to understand human and machine intelligence.\n",
      "\n",
      "For example, it may be possible to build artificial intelligence systems to improve our understanding of mental health and other social problems at the same time. It could also be used to solve problems of social order and to overcome problems of economic inequality.\n",
      "\n",
      "But most important, it may also\n",
      "\n",
      "Sample 2:\n",
      "In the future, artificial intelligence will be a major player in the field of medicine.\n",
      "\n",
      "In the future, artificial intelligence will be a major player in the field of medicine.\n",
      "\n",
      "Sample 3:\n",
      "In the future, artificial intelligence will be used to assist scientists in predicting what the future may hold for life on Earth.\n",
      "\n",
      "The US space agency is also working on a robot that could someday be part of the search for life on Mars and beyond.\n",
      "\n",
      "The International Space Station's robotic arm, known as the ISS-B, is currently testing out its own version of the technology, which\n"
     ]
    }
   ],
   "source": [
    "# Top-k sampling: choose from top-k probable tokens\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=80,\n",
    "    do_sample=True,\n",
    "    top_k=50,         # only sample from top 50 tokens\n",
    "    temperature=0.7,  # randomness control\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "for i, sample in enumerate(outputs):\n",
    "    print(f\"\\nSample {i+1}:\\n{tokenizer.decode(sample, skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23c714db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "In the future, artificial intelligence will become the dominant tool for all forms of technology that will allow us to make decisions about what to do with our lives. It is true that AI may not be a sure thing for all of us, but it is a promising tool for humans and humans will continue to evolve and evolve with the technologies that we use in everyday life. It is also true that we are\n",
      "\n",
      "Sample 2:\n",
      "In the future, artificial intelligence will be able to learn and be used by people on the planet, and it will be able to be used for scientific research,\" said Dr. C. L. van der Laan, a physicist at the University of Cambridge who is leading the study. \"This will mean that we have a real opportunity to develop the kind of systems we want to be able to understand\n",
      "\n",
      "Sample 3:\n",
      "In the future, artificial intelligence will help us to navigate the world in a more efficient way. Artificial intelligence is also making us more aware of our surroundings and we will be able to better plan for them. And we will be able to build better cars and better roads. And we will be able to learn more about things like human emotions and how we feel. It is not just about computers but we\n"
     ]
    }
   ],
   "source": [
    "# Top-p sampling: dynamic pool until cumulative probability p\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=80,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,        # select tokens covering 90% cumulative probability\n",
    "    temperature=0.8,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "for i, sample in enumerate(outputs):\n",
    "    print(f\"\\nSample {i+1}:\\n{tokenizer.decode(sample, skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16539543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam search output:\n",
      "In the future, artificial intelligence will be able to do a lot of things that humans can't. It will be able to do a lot of things that humans can't do. It will be able to do a lot of things that humans can't\n"
     ]
    }
   ],
   "source": [
    "# Beam search: keep multiple candidate sequences\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    num_beams=5,         # number of beams\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Beam search output:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
